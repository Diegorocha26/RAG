# RAG Evaluations

1. Curate a Test Set
Pre-made questions and answers with keywords that should be in the relevant context.

    - Where to get them from:
        - Take them from your data / info
        - From the user questions if the system has already been in production

2. Measure Retrieval
How good is the system at retrieving relevant content to feed the LLM.

    - Popular & Useful Metrics:
        - MRR (Mean Reciprocal Rank): 0-1 scale, average inverse rank of first hit; 1 if the first chunk always has relevant context 
        - nDCG (Normalized Discounted Cumulative Gain): how good is the system at surfacing relevant context to the top chunks; i.e. did relevant chunks get ranked higher up
        - Recall@K: proportion of tests where relevant context was in the top K chunks; or if you have multiple keywords to look for, 'keyword coverage' is a similar recall metric
        - Precision@K: proportion of the top K chunks that are relevant

3. Measure Answers
Compare the real answer of the question to answer generated by the LLM.

    - Approach 1: Use LLM-as-a-judge to score provided answers against criteria like accuracy, completeness, and relevance.
    - Approach 2: Vectorize both answers and see how similar they are.

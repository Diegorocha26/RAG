{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a89a791a",
   "metadata": {},
   "source": [
    "# RAG Evaluation Notebook\n",
    "\n",
    "This notebook evaluates the performance of a Retrieval-Augmented Generation (RAG) system. It loads test cases and measures:\n",
    "- **Retrieval quality**: How well the system retrieves relevant documents\n",
    "- **Answer quality**: How accurate, complete, and relevant the generated answers are\n",
    "\n",
    "The evaluation framework uses predefined test cases with reference answers and expected keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab894b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Python path to access project modules\n",
    "import sys\n",
    "import os\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\", \"..\"))\n",
    "sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accea641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the test loader to access the evaluation test cases\n",
    "from interface_v1.evaluations.test_loader import load_tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4183ad4",
   "metadata": {},
   "source": [
    "## Step 1: Load Test Cases\n",
    "\n",
    "Load the test dataset containing questions, reference answers, categories, and keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e76c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all test cases from the test dataset\n",
    "tests = load_tests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf4edc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the total number of test cases\n",
    "len(tests)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da22e42d",
   "metadata": {},
   "source": [
    "## Step 2: Inspect Test Data\n",
    "\n",
    "Each test case contains:\n",
    "- **question**: The user query\n",
    "- **category**: The topic/domain (e.g., insurance product, company info)\n",
    "- **reference_answer**: The ground truth answer\n",
    "- **keywords**: Expected terms that should appear in correct answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b3770d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the structure of a test case\n",
    "example = tests[0]\n",
    "print(f\"Question: {example.question}\")\n",
    "print(f\"Category: {example.category}\")\n",
    "print(f\"Reference Answer: {example.reference_answer}\")\n",
    "print(f\"Keywords: {example.keywords}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8598e815",
   "metadata": {},
   "source": [
    "## Step 3: Analyze Test Distribution\n",
    "\n",
    "Check how many test cases exist for each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07537c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the distribution of test cases by category\n",
    "from collections import Counter\n",
    "count = Counter([t.category for t in tests])\n",
    "count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a441f02c",
   "metadata": {},
   "source": [
    "## Step 4: Evaluate RAG System Performance\n",
    "\n",
    "Use evaluation functions to measure retrieval quality and answer generation quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d154c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import evaluation functions for retrieval and answer quality assessment\n",
    "from interface_v1.evaluations.evaluations import evaluate_retrieval, evaluate_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6866d83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate how well the system retrieves relevant documents for the example query\n",
    "evaluate_retrieval(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f3c5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the quality of the generated answer, including accuracy, completeness, and relevance\n",
    "eval, answer, chunks = evaluate_answer(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d720022c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the evaluation results\n",
    "print(answer)\n",
    "print(eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb54fa5",
   "metadata": {},
   "source": [
    "## Evaluation Metrics\n",
    "\n",
    "The evaluation provides the following metrics:\n",
    "- **Accuracy**: How factually correct the answer is (0-1 scale)\n",
    "- **Completeness**: How thoroughly the question is answered (0-1 scale)\n",
    "- **Relevance**: How relevant the retrieved documents are to the query (0-1 scale)\n",
    "- **Feedback**: Detailed comments on strengths and areas for improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c56486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display detailed evaluation metrics\n",
    "print(f\"Feedback: {eval.feedback}\")\n",
    "print(f\"Accuracy: {eval.accuracy}\")\n",
    "print(f\"Completeness: {eval.completeness}\")\n",
    "print(f\"Relevance: {eval.relevance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5058a631",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "rag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

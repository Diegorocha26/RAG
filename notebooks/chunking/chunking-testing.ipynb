{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4",
   "metadata": {},
   "source": [
    "# Chunking Testing Notebook\n",
    "An isolated environment to experiment with different document loaders, chunking strategies, and parameters — and visualize the results before committing to an approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c3d4e5",
   "metadata": {},
   "source": [
    "## Part 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d4e5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from IPython.display import display, HTML\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain_text_splitters import (\n",
    "    RecursiveCharacterTextSplitter,\n",
    "    CharacterTextSplitter,\n",
    "    MarkdownHeaderTextSplitter,\n",
    "    TokenTextSplitter,\n",
    ")\n",
    "\n",
    "# ── Tokenizer (swap model name to match your LLM) ──────────────────────────\n",
    "TOKENIZER_MODEL = \"gpt-4o\"   # used only for token counting, not for embeddings\n",
    "encoding = tiktoken.encoding_for_model(TOKENIZER_MODEL)\n",
    "\n",
    "def count_tokens(text: str) -> int:\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "print(\"Setup complete ✓\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e5f6a7",
   "metadata": {},
   "source": [
    "## Part 2: Load Documents\n",
    "\n",
    "> **TIP — Swapping loaders:** This cell uses `DirectoryLoader + TextLoader` by default.\n",
    "> To try a different loader, comment out the current block and uncomment one of the alternatives below.\n",
    "> All loaders produce a list of `Document` objects with `.page_content` and `.metadata`, so the rest of the notebook works unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f6a7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── CONFIGURE: point this at your data ────────────────────────────────────\n",
    "DATA_PATH = \"../../data/knowledge-base\"\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# ══════════════════════════════════════════════════════════════════════════\n",
    "# LOADER OPTION 1 (active): DirectoryLoader + TextLoader\n",
    "# Good for: plain text, markdown files organised in folders.\n",
    "# ══════════════════════════════════════════════════════════════════════════\n",
    "folders = glob.glob(os.path.join(DATA_PATH, \"*\"))\n",
    "documents = []\n",
    "for folder in folders:\n",
    "    doc_type = os.path.basename(folder)\n",
    "    loader = DirectoryLoader(\n",
    "        folder,\n",
    "        glob=\"**/*.md\",\n",
    "        loader_cls=TextLoader,\n",
    "        loader_kwargs={\"encoding\": \"utf-8\"},\n",
    "    )\n",
    "    for doc in loader.load():\n",
    "        doc.metadata[\"doc_type\"] = doc_type\n",
    "        documents.append(doc)\n",
    "\n",
    "\n",
    "# ══════════════════════════════════════════════════════════════════════════\n",
    "# LOADER OPTION 2 (swap in): UnstructuredMarkdownLoader\n",
    "# Good for: richer markdown parsing (tables, headers preserved as elements).\n",
    "# ══════════════════════════════════════════════════════════════════════════\n",
    "# from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
    "# files = glob.glob(os.path.join(DATA_PATH, \"**/*.md\"), recursive=True)\n",
    "# documents = []\n",
    "# for fp in files:\n",
    "#     docs = UnstructuredMarkdownLoader(fp).load()\n",
    "#     for doc in docs:\n",
    "#         doc.metadata[\"doc_type\"] = os.path.basename(os.path.dirname(fp))\n",
    "#         documents.append(doc)\n",
    "\n",
    "\n",
    "# ══════════════════════════════════════════════════════════════════════════\n",
    "# LOADER OPTION 3 (swap in): PyPDFLoader\n",
    "# Good for: PDFs — each page becomes a Document.\n",
    "# ══════════════════════════════════════════════════════════════════════════\n",
    "# from langchain_community.document_loaders import PyPDFLoader\n",
    "# files = glob.glob(os.path.join(DATA_PATH, \"**/*.pdf\"), recursive=True)\n",
    "# documents = []\n",
    "# for fp in files:\n",
    "#     docs = PyPDFLoader(fp).load()\n",
    "#     for doc in docs:\n",
    "#         doc.metadata[\"doc_type\"] = os.path.basename(os.path.dirname(fp))\n",
    "#         documents.append(doc)\n",
    "\n",
    "\n",
    "# ══════════════════════════════════════════════════════════════════════════\n",
    "# LOADER OPTION 4 (swap in): CSVLoader\n",
    "# Good for: structured/tabular data — each row becomes a Document.\n",
    "# ══════════════════════════════════════════════════════════════════════════\n",
    "# from langchain_community.document_loaders import CSVLoader\n",
    "# files = glob.glob(os.path.join(DATA_PATH, \"**/*.csv\"), recursive=True)\n",
    "# documents = []\n",
    "# for fp in files:\n",
    "#     docs = CSVLoader(fp).load()\n",
    "#     for doc in docs:\n",
    "#         doc.metadata[\"doc_type\"] = os.path.basename(os.path.dirname(fp))\n",
    "#         documents.append(doc)\n",
    "\n",
    "\n",
    "# ══════════════════════════════════════════════════════════════════════════\n",
    "print(f\"Loaded {len(documents)} documents\")\n",
    "total_chars = sum(len(d.page_content) for d in documents)\n",
    "total_tokens = sum(count_tokens(d.page_content) for d in documents)\n",
    "print(f\"Total characters : {total_chars:,}\")\n",
    "print(f\"Total tokens     : {total_tokens:,}  (model: {TOKENIZER_MODEL})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a7b8c9",
   "metadata": {},
   "source": [
    "### Quick peek at a raw document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b8c9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOC_INDEX = 0   # ← change to inspect a different document\n",
    "\n",
    "doc = documents[DOC_INDEX]\n",
    "print(f\"Metadata : {doc.metadata}\")\n",
    "print(f\"Characters: {len(doc.page_content):,}\")\n",
    "print(f\"Tokens    : {count_tokens(doc.page_content):,}\")\n",
    "print(\"─\" * 60)\n",
    "print(doc.page_content[:2000])\n",
    "if len(doc.page_content) > 2000:\n",
    "    print(\"\\n... [truncated — set DOC_INDEX or slice further to see more]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c9d0e1",
   "metadata": {},
   "source": [
    "## Part 3: Define Chunking Strategies\n",
    "\n",
    "> **TIP:** Edit the `STRATEGIES` dict to add, remove, or tweak any strategy.\n",
    "> Each entry is just a LangChain splitter instance — the analysis cells below will pick them all up automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d0e1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── CONFIGURE: add / edit / comment out strategies here ────────────────────\n",
    "STRATEGIES = {\n",
    "\n",
    "    # ── Active strategies ──────────────────────────────────────────────────\n",
    "\n",
    "    \"Recursive · 1000/200\": RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200,\n",
    "    ),\n",
    "\n",
    "    # ── Swap in / experiment with any of the below ─────────────────────────\n",
    "\n",
    "    # \"Recursive · 500/50\": RecursiveCharacterTextSplitter(\n",
    "    #     chunk_size=500, chunk_overlap=50\n",
    "    # ),\n",
    "\n",
    "    # \"Recursive · 2000/400\": RecursiveCharacterTextSplitter(\n",
    "    #     chunk_size=2000, chunk_overlap=400\n",
    "    # ),\n",
    "\n",
    "    # \"Character · newline\": CharacterTextSplitter(\n",
    "    #     separator=\"\\n\", chunk_size=1000, chunk_overlap=200\n",
    "    # ),\n",
    "\n",
    "    # \"Token · 256/32\": TokenTextSplitter(\n",
    "    #     chunk_size=256, chunk_overlap=32\n",
    "    # ),\n",
    "\n",
    "    # \"Markdown Headers\": MarkdownHeaderTextSplitter(\n",
    "    #     headers_to_split_on=[(\"#\", \"h1\"), (\"##\", \"h2\"), (\"###\", \"h3\")]\n",
    "    # ),\n",
    "}\n",
    "# ──────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "\n",
    "# Run all strategies and store results\n",
    "results = {}\n",
    "for name, splitter in STRATEGIES.items():\n",
    "    # MarkdownHeaderTextSplitter takes raw text, not Documents\n",
    "    if isinstance(splitter, MarkdownHeaderTextSplitter):\n",
    "        chunks = []\n",
    "        for doc in documents:\n",
    "            split_docs = splitter.split_text(doc.page_content)\n",
    "            for sd in split_docs:\n",
    "                sd.metadata.update(doc.metadata)\n",
    "                chunks.append(sd)\n",
    "    else:\n",
    "        chunks = splitter.split_documents(documents)\n",
    "\n",
    "    sizes = [len(c.page_content) for c in chunks]\n",
    "    tokens = [count_tokens(c.page_content) for c in chunks]\n",
    "    results[name] = {\"chunks\": chunks, \"sizes\": sizes, \"tokens\": tokens}\n",
    "    print(f\"{name:35s} → {len(chunks):>5} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e1f2a3",
   "metadata": {},
   "source": [
    "## Part 4: Strategy Comparison Table\n",
    "High-level stats across all active strategies at a glance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f2a3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for name, r in results.items():\n",
    "    s, t = r[\"sizes\"], r[\"tokens\"]\n",
    "    rows.append({\n",
    "        \"Strategy\"       : name,\n",
    "        \"# Chunks\"       : len(s),\n",
    "        \"Avg chars\"      : int(np.mean(s)),\n",
    "        \"Min chars\"      : min(s),\n",
    "        \"Max chars\"      : max(s),\n",
    "        \"Avg tokens\"     : int(np.mean(t)),\n",
    "        \"Min tokens\"     : min(t),\n",
    "        \"Max tokens\"     : max(t),\n",
    "        \"Total tokens\"   : sum(t),\n",
    "    })\n",
    "\n",
    "df_compare = pd.DataFrame(rows).set_index(\"Strategy\")\n",
    "display(df_compare.style.background_gradient(cmap=\"Blues\", subset=[\"# Chunks\", \"Avg chars\", \"Avg tokens\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a3b4c5",
   "metadata": {},
   "source": [
    "## Part 5: Chunk Size Distribution\n",
    "Spot outliers — chunks that are suspiciously tiny or unexpectedly large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b4c5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "for name, r in results.items():\n",
    "    fig.add_trace(go.Histogram(\n",
    "        x=r[\"sizes\"],\n",
    "        name=name,\n",
    "        opacity=0.7,\n",
    "        nbinsx=40,\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Chunk Size Distribution (characters)\",\n",
    "    barmode=\"overlay\",\n",
    "    xaxis_title=\"Characters per chunk\",\n",
    "    yaxis_title=\"Number of chunks\",\n",
    "    width=900, height=450,\n",
    "    legend=dict(x=0.75, y=0.95),\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c5d6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same plot but for TOKENS\n",
    "fig = go.Figure()\n",
    "for name, r in results.items():\n",
    "    fig.add_trace(go.Histogram(\n",
    "        x=r[\"tokens\"],\n",
    "        name=name,\n",
    "        opacity=0.7,\n",
    "        nbinsx=40,\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Chunk Token Distribution\",\n",
    "    barmode=\"overlay\",\n",
    "    xaxis_title=\"Tokens per chunk\",\n",
    "    yaxis_title=\"Number of chunks\",\n",
    "    width=900, height=450,\n",
    "    legend=dict(x=0.75, y=0.95),\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d6e7f8",
   "metadata": {},
   "source": [
    "## Part 6: Overlap Visualizer\n",
    "Highlights the shared text between consecutive chunks so you can *see* your overlap strategy working (or not)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e7f8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_overlap(text_a: str, text_b: str) -> str:\n",
    "    \"\"\"Return the longest suffix of text_a that is a prefix of text_b.\"\"\"\n",
    "    max_overlap = min(len(text_a), len(text_b))\n",
    "    for size in range(max_overlap, 0, -1):\n",
    "        if text_a[-size:] == text_b[:size]:\n",
    "            return text_a[-size:]\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def render_overlap_html(chunk_a: str, chunk_b: str, overlap: str) -> str:\n",
    "    \"\"\"Return an HTML string highlighting the overlapping region.\"\"\"\n",
    "    def _highlight(text, overlap, position):\n",
    "        if not overlap:\n",
    "            return f\"<pre style='white-space:pre-wrap;font-size:13px'>{text}</pre>\"\n",
    "        if position == \"tail\":\n",
    "            idx = text.rfind(overlap)\n",
    "        else:\n",
    "            idx = text.find(overlap)\n",
    "        if idx == -1:\n",
    "            return f\"<pre style='white-space:pre-wrap;font-size:13px'>{text}</pre>\"\n",
    "        before = text[:idx]\n",
    "        after  = text[idx + len(overlap):]\n",
    "        return (\n",
    "            f\"<pre style='white-space:pre-wrap;font-size:13px'>\"\n",
    "            f\"{before}\"\n",
    "            f\"<mark style='background:#ffe066;padding:0'>{overlap}</mark>\"\n",
    "            f\"{after}</pre>\"\n",
    "        )\n",
    "\n",
    "    html = (\n",
    "        \"<div style='display:flex;gap:16px'>\"\n",
    "        \"<div style='flex:1;border:1px solid #ccc;border-radius:6px;padding:12px'>\"\n",
    "        f\"<b>Chunk A</b> ({len(chunk_a)} chars)\"\n",
    "        + _highlight(chunk_a, overlap, \"tail\") +\n",
    "        \"</div>\"\n",
    "        \"<div style='flex:1;border:1px solid #ccc;border-radius:6px;padding:12px'>\"\n",
    "        f\"<b>Chunk B</b> ({len(chunk_b)} chars)\"\n",
    "        + _highlight(chunk_b, overlap, \"head\") +\n",
    "        \"</div>\"\n",
    "        \"</div>\"\n",
    "        f\"<p style='color:#666;font-size:12px'>Overlap: <b>{len(overlap)}</b> characters highlighted in yellow</p>\"\n",
    "    )\n",
    "    return html\n",
    "\n",
    "\n",
    "print(\"Overlap visualizer ready — run the cell below to inspect a pair of chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f8a9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── CONFIGURE ──────────────────────────────────────────────────────────────\n",
    "STRATEGY_TO_INSPECT = list(STRATEGIES.keys())[0]   # ← swap strategy name here\n",
    "CHUNK_INDEX         = 0                             # ← inspect chunks N and N+1\n",
    "# ──────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "chunks = results[STRATEGY_TO_INSPECT][\"chunks\"]\n",
    "ca, cb = chunks[CHUNK_INDEX].page_content, chunks[CHUNK_INDEX + 1].page_content\n",
    "overlap = find_overlap(ca, cb)\n",
    "display(HTML(render_overlap_html(ca, cb, overlap)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a9b0c1",
   "metadata": {},
   "source": [
    "## Part 7: Raw Chunk Inspector\n",
    "Page through individual chunks to read the actual text and evaluate quality directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b0c1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── CONFIGURE ──────────────────────────────────────────────────────────────\n",
    "STRATEGY_TO_INSPECT = list(STRATEGIES.keys())[0]   # ← swap strategy name here\n",
    "CHUNK_INDEX         = 0                             # ← which chunk to display\n",
    "# ──────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "chunks = results[STRATEGY_TO_INSPECT][\"chunks\"]\n",
    "chunk  = chunks[CHUNK_INDEX]\n",
    "\n",
    "print(f\"Strategy : {STRATEGY_TO_INSPECT}\")\n",
    "print(f\"Chunk    : {CHUNK_INDEX + 1} / {len(chunks)}\")\n",
    "print(f\"Chars    : {len(chunk.page_content):,}\")\n",
    "print(f\"Tokens   : {count_tokens(chunk.page_content):,}\")\n",
    "print(f\"Metadata : {chunk.metadata}\")\n",
    "print(\"─\" * 60)\n",
    "print(chunk.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c1d2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Batch inspector: display multiple chunks at once ───────────────────────\n",
    "# ── CONFIGURE ──────────────────────────────────────────────────────────────\n",
    "STRATEGY_TO_INSPECT = list(STRATEGIES.keys())[0]\n",
    "START_INDEX = 0\n",
    "END_INDEX   = 5    # exclusive — shows chunks START_INDEX to END_INDEX - 1\n",
    "# ──────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "chunks = results[STRATEGY_TO_INSPECT][\"chunks\"]\n",
    "rows_html = \"\"\n",
    "for i, chunk in enumerate(chunks[START_INDEX:END_INDEX], start=START_INDEX):\n",
    "    rows_html += (\n",
    "        f\"<tr>\"\n",
    "        f\"<td style='padding:6px;vertical-align:top;font-weight:bold'>#{i}</td>\"\n",
    "        f\"<td style='padding:6px;vertical-align:top'>{chunk.metadata.get('doc_type','—')}</td>\"\n",
    "        f\"<td style='padding:6px;vertical-align:top'>{len(chunk.page_content):,}</td>\"\n",
    "        f\"<td style='padding:6px;vertical-align:top'>{count_tokens(chunk.page_content):,}</td>\"\n",
    "        f\"<td style='padding:6px;vertical-align:top;white-space:pre-wrap;font-size:12px;max-width:600px'>\"\n",
    "        f\"{chunk.page_content[:500]}{'...' if len(chunk.page_content) > 500 else ''}\"\n",
    "        f\"</td>\"\n",
    "        f\"</tr>\"\n",
    "    )\n",
    "\n",
    "html = (\n",
    "    f\"<h4>{STRATEGY_TO_INSPECT} — chunks {START_INDEX}–{END_INDEX-1}</h4>\"\n",
    "    \"<table border='1' style='border-collapse:collapse;font-size:13px;width:100%'>\"\n",
    "    \"<tr style='background:#f0f0f0'>\"\n",
    "    \"<th>Index</th><th>Doc type</th><th>Chars</th><th>Tokens</th><th>Content preview</th>\"\n",
    "    \"</tr>\"\n",
    "    + rows_html +\n",
    "    \"</table>\"\n",
    ")\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d2e3f4",
   "metadata": {},
   "source": [
    "## Part 8: Find Outlier Chunks\n",
    "Surface the smallest and largest chunks — useful for spotting broken splits or runaway chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e3f4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── CONFIGURE ──────────────────────────────────────────────────────────────\n",
    "STRATEGY_TO_INSPECT = list(STRATEGIES.keys())[0]\n",
    "TOP_N = 5   # how many smallest and largest to show\n",
    "# ──────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "chunks = results[STRATEGY_TO_INSPECT][\"chunks\"]\n",
    "indexed = sorted(enumerate(chunks), key=lambda x: len(x[1].page_content))\n",
    "\n",
    "print(f\"{'─'*60}\\n  {TOP_N} SMALLEST chunks — {STRATEGY_TO_INSPECT}\\n{'─'*60}\")\n",
    "for idx, chunk in indexed[:TOP_N]:\n",
    "    print(f\"\\n[Chunk #{idx} | {len(chunk.page_content)} chars | {count_tokens(chunk.page_content)} tokens]\")\n",
    "    print(chunk.page_content)\n",
    "\n",
    "print(f\"\\n{'─'*60}\\n  {TOP_N} LARGEST chunks — {STRATEGY_TO_INSPECT}\\n{'─'*60}\")\n",
    "for idx, chunk in indexed[-TOP_N:][::-1]:\n",
    "    print(f\"\\n[Chunk #{idx} | {len(chunk.page_content)} chars | {count_tokens(chunk.page_content)} tokens]\")\n",
    "    print(chunk.page_content[:500], \"...\" if len(chunk.page_content) > 500 else \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f4a5b6",
   "metadata": {},
   "source": [
    "## Part 9: Search Within Chunks\n",
    "Check whether a specific concept, term, or sentence ended up split across chunks or kept intact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a5b6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── CONFIGURE ──────────────────────────────────────────────────────────────\n",
    "STRATEGY_TO_INSPECT = list(STRATEGIES.keys())[0]\n",
    "SEARCH_TERM = \"your search term here\"   # ← case-insensitive substring match\n",
    "# ──────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "chunks   = results[STRATEGY_TO_INSPECT][\"chunks\"]\n",
    "matches  = [(i, c) for i, c in enumerate(chunks) if SEARCH_TERM.lower() in c.page_content.lower()]\n",
    "\n",
    "print(f\"Found '{SEARCH_TERM}' in {len(matches)} / {len(chunks)} chunks\\n\")\n",
    "for idx, chunk in matches:\n",
    "    text = chunk.page_content\n",
    "    pos  = text.lower().find(SEARCH_TERM.lower())\n",
    "    snippet_start = max(0, pos - 100)\n",
    "    snippet_end   = min(len(text), pos + len(SEARCH_TERM) + 100)\n",
    "    snippet = text[snippet_start:snippet_end]\n",
    "    print(f\"─── Chunk #{idx} ({len(text)} chars) ───\")\n",
    "    print(\"...\" if snippet_start > 0 else \"\", snippet, \"...\" if snippet_end < len(text) else \"\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "rag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": 5,
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
